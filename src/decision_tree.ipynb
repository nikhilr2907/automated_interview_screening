{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Baseline Model Comparison (Decision Tree)\n",
    "\n",
    "In this notebook, you will train a Decision Tree model to predict the `prior_hiring_decision` target variable. You should compare its performance (Accuracy and Fairness) to the Logistic Regression baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Equalized Odds**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalized_odds(data, pred_col='prediction', true_col='prior_hiring_decision'):\n",
    "    \"\"\"\n",
    "    Compute equalized odds for binary race (white=1 vs non-white=anything else).\n",
    "    TPR = P(pred=1 | true=1), FPR = P(pred=1 | true=0)\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    data['_race_binary'] = (data['Race'] == 1).astype(int)\n",
    "    \n",
    "    tpr_rates = []\n",
    "    fpr_rates = []\n",
    "    \n",
    "    for race in [0, 1]:  # 0=non-white, 1=white\n",
    "        group_data = data[data['_race_binary'] == race]\n",
    "        \n",
    "        # Actual positives and negatives\n",
    "        actual_positives = group_data[group_data[true_col] == 1]\n",
    "        actual_negatives = group_data[group_data[true_col] == 0]\n",
    "        \n",
    "        # TPR: of actual positives, how many predicted positive?\n",
    "        if len(actual_positives) > 0:\n",
    "            tpr = actual_positives[pred_col].mean()\n",
    "            tpr_rates.append(tpr)\n",
    "        \n",
    "        # FPR: of actual negatives, how many predicted positive?\n",
    "        if len(actual_negatives) > 0:\n",
    "            fpr = actual_negatives[pred_col].mean()\n",
    "            fpr_rates.append(fpr)\n",
    "    \n",
    "    max_tpr, min_tpr = max(tpr_rates), min(tpr_rates)\n",
    "    max_fpr, min_fpr = max(fpr_rates), min(fpr_rates)\n",
    "    \n",
    "    tpr_ratio = float('inf') if min_tpr == 0 else max_tpr / min_tpr\n",
    "    fpr_ratio = float('inf') if min_fpr == 0 else max_fpr / min_fpr\n",
    "    \n",
    "    return max(tpr_ratio, fpr_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mode: UNWEIGHTED\n",
      "Training Shape: (30000, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Data\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "val_df = pd.read_csv('../data/val.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "\n",
    "# Configuration: Toggle between weighted and unweighted training\n",
    "USE_WEIGHTED = False\n",
    "\n",
    "columns_to_drop = [\"Hours_Per_Week\", \"Marital_Status\", \"Relationship\"]\n",
    "protected_characteristics = [\"Sex\", \"Race\", \"Age\", \"Place_Of_Birth\"]\n",
    "target = 'prior_hiring_decision'\n",
    "\n",
    "# Drop columns_to_drop (but keep protected characteristics for now - needed for weights)\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "print(f\"Training mode: {'WEIGHTED' if USE_WEIGHTED else 'UNWEIGHTED'}\")\n",
    "print(\"Training Shape:\", train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Weights (applied to loss function):\n",
      "  Formula: w = P(race) * P(label) / P(race, label)\n",
      "\n",
      "  non-white, not hired: 0.8388\n",
      "  non-white, hired: 1.2625\n",
      "  white, not hired: 1.1321\n",
      "  white, hired: 0.8879\n"
     ]
    }
   ],
   "source": [
    "def compute_group_weights(df, label_col):\n",
    "    \"\"\"\n",
    "    Compute rebalancing weights for each (race_binary, label) combination.\n",
    "    Returns a dictionary mapping (race_binary, label) -> weight.\n",
    "    \n",
    "    Weight formula: w = P(race) * P(label) / P(race, label)\n",
    "    This makes race and label independent in the weighted distribution.\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    labels = df[label_col].unique()\n",
    "    \n",
    "    # Binary race: 1 = white, 0 = non-white\n",
    "    race_binary = (df['Race'] == 1).astype(int)\n",
    "    \n",
    "    weights = {}\n",
    "    for race in [0, 1]:\n",
    "        race_count = (race_binary == race).sum()\n",
    "        for label in labels:\n",
    "            label_count = (df[label_col] == label).sum()\n",
    "            intersection_count = ((race_binary == race) & (df[label_col] == label)).sum()\n",
    "            \n",
    "            if intersection_count > 0:\n",
    "                # w = P(race) * P(label) / P(race, label)\n",
    "                weight = (race_count * label_count) / (total * intersection_count)\n",
    "            else:\n",
    "                weight = 1.0\n",
    "            \n",
    "            weights[(race, int(label))] = float(weight)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "\n",
    "def get_sample_weights(df, group_weights, label_col):\n",
    "    \"\"\"\n",
    "    Get per-sample weights array for use in loss function.\n",
    "    \"\"\"\n",
    "    race_binary = (df['Race'] == 1).astype(int)\n",
    "    sample_weights = [\n",
    "        group_weights.get((race, int(label)), 1.0)\n",
    "        for race, label in zip(race_binary, df[label_col])\n",
    "    ]\n",
    "    return np.array(sample_weights)\n",
    "\n",
    "\n",
    "# Compute group weights from training data\n",
    "group_weights = compute_group_weights(train_df, target)\n",
    "\n",
    "print(\"Group Weights (applied to loss function):\")\n",
    "print(\"  Formula: w = P(race) * P(label) / P(race, label)\")\n",
    "print()\n",
    "for (race, label), weight in sorted(group_weights.items()):\n",
    "    race_name = \"white\" if race == 1 else \"non-white\"\n",
    "    label_name = \"hired\" if label == 1 else \"not hired\"\n",
    "    print(f\"  {race_name}, {label_name}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with NO WEIGHTS via GridSearchCV...\n",
      "\n",
      "Best parameters: {'classifier__min_samples_split': 5, 'classifier__min_samples_leaf': 4, 'classifier__max_depth': None}\n",
      "Internal CV Score (Mean): 0.6705\n",
      "Final Eval Accuracy (on X_val): 0.6718\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare sample weights\n",
    "if USE_WEIGHTED:\n",
    "    sample_weights = get_sample_weights(train_df, group_weights, target)\n",
    "else:\n",
    "    sample_weights = None\n",
    "\n",
    "# 2. Define features and split (using raw DataFrames)\n",
    "cols_to_exclude = protected_characteristics + [target]\n",
    "features = [col for col in train_df.columns if col not in cols_to_exclude]\n",
    "\n",
    "X_train, y_train = train_df[features], train_df[target]\n",
    "X_val, y_val = val_df[features], val_df[target]\n",
    "x_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "# 3. Build the Pipeline\n",
    "# We keep the raw data until it enters the pipeline\n",
    "categorical_cols = [col for col in ['Workclass', 'Education', 'Occupation'] if col in features]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# 4. Hyperparameter grid (Note the 'classifier__' prefix)\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [10, 15, None],\n",
    "    'classifier__min_samples_split': [5, 10],\n",
    "    'classifier__min_samples_leaf': [2, 4]\n",
    "}\n",
    "\n",
    "print(f\"\\nTraining with {'SAMPLE WEIGHTS' if USE_WEIGHTED else 'NO WEIGHTS'} via GridSearchCV...\\n\")\n",
    "\n",
    "# 5. Execute Grid Search\n",
    "# CV=5 provides a more robust estimate of performance than a single validation split\n",
    "random_search = RandomizedSearchCV(\n",
    "    full_pipeline,\n",
    "    param_grid,\n",
    "    n_iter=1,      \n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42 \n",
    ")\n",
    "\n",
    "# Fit using raw X_train; weights are passed directly to the classifier step\n",
    "random_search.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    classifier__sample_weight=sample_weights\n",
    ")\n",
    "\n",
    "# 6. Final Evaluation\n",
    "best_pipe = random_search.best_estimator_\n",
    "val_accuracy = best_pipe.score(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Internal CV Score (Mean): {random_search.best_score_:.4f}\")\n",
    "print(f\"Final Eval Accuracy (on X_train): {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6718\n",
      "Confusion Matrix:\n",
      "[[3764 1435]\n",
      " [1847 2954]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_pipe.predict(x_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fairness Metric Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness Metrics (Decision Tree) - Binary Race:\n",
      "\n",
      "Equalized Odds Ratio: 1.6178\n",
      "\n",
      "By Group:\n",
      "  non-white: TPR=0.5403, FPR=0.2049, Count=3756\n",
      "  white: TPR=0.6486, FPR=0.3315, Count=6244\n"
     ]
    }
   ],
   "source": [
    "val_with_preds = val_df.copy()\n",
    "val_with_preds['prediction'] = best_pipe.predict(X_val)\n",
    "\n",
    "# Compute equalized odds for binary race (white vs non-white)\n",
    "eo_ratio = equalized_odds(val_with_preds, pred_col='prediction', true_col=target)\n",
    "\n",
    "# Get detailed TPR/FPR by group\n",
    "val_with_preds['_race_binary'] = (val_with_preds['Race'] == 1).astype(int)\n",
    "results = {}\n",
    "for race in [0, 1]:\n",
    "    group_data = val_with_preds[val_with_preds['_race_binary'] == race]\n",
    "    \n",
    "    # Actual positives and negatives (based on true label)\n",
    "    actual_positives = group_data[group_data[target] == 1]\n",
    "    actual_negatives = group_data[group_data[target] == 0]\n",
    "    \n",
    "    race_name = \"white\" if race == 1 else \"non-white\"\n",
    "    results[race_name] = {\n",
    "        # TPR: of actual positives, fraction predicted positive\n",
    "        'tpr': float(actual_positives['prediction'].mean()) if len(actual_positives) > 0 else None,\n",
    "        # FPR: of actual negatives, fraction predicted positive\n",
    "        'fpr': float(actual_negatives['prediction'].mean()) if len(actual_negatives) > 0 else None,\n",
    "        'count': len(group_data)\n",
    "    }\n",
    "\n",
    "print(\"Fairness Metrics (Decision Tree) - Binary Race:\")\n",
    "print(f\"\\nEqualized Odds Ratio: {eo_ratio:.4f}\")\n",
    "print(\"\\nBy Group:\")\n",
    "for group, metrics in results.items():\n",
    "    print(f\"  {group}: TPR={metrics['tpr']:.4f}, FPR={metrics['fpr']:.4f}, Count={metrics['count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ROC Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_curve, roc_auc_score\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m y_proba = \u001b[43mbest_pipe\u001b[49m.predict_proba(x_test)[:, \u001b[32m1\u001b[39m]\n\u001b[32m      5\u001b[39m fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n\u001b[32m      6\u001b[39m auc_score = roc_auc_score(y_test, y_proba)\n",
      "\u001b[31mNameError\u001b[39m: name 'best_pipe' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_proba = best_pipe.predict_proba(x_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "auc_score = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'Decision Tree (AUC = {auc_score:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Decision Tree')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Adjusted Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_equalized_odds(data, y_true_col, y_proba, thresholds):\n",
    "    \"\"\"\n",
    "    Calculate equalized odds using group-specific classification thresholds.\n",
    "    Uses binary race: white (Race==1) vs non-white (Race!=1).\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with Race column and true labels\n",
    "        y_true_col: Name of true label column\n",
    "        y_proba: Array of predicted probabilities\n",
    "        thresholds: Dict mapping race_binary (0 or 1) to classification threshold\n",
    "                    e.g., {0: 0.4, 1: 0.6} means non-white uses 0.4, white uses 0.6\n",
    "    \n",
    "    Returns:\n",
    "        Dict with TPR/FPR per group and equalized odds ratio\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    data['y_proba'] = y_proba\n",
    "    data['_race_binary'] = (data['Race'] == 1).astype(int)\n",
    "    \n",
    "    # Apply group-specific thresholds\n",
    "    data['adjusted_pred'] = data.apply(\n",
    "        lambda row: 1 if row['y_proba'] >= thresholds.get(row['_race_binary'], 0.5) else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    tpr_rates = []\n",
    "    fpr_rates = []\n",
    "    results = {}\n",
    "    \n",
    "    for race in [0, 1]:  # 0=non-white, 1=white\n",
    "        group_data = data[data['_race_binary'] == race]\n",
    "        actual_positives = group_data[group_data[y_true_col] == 1]\n",
    "        actual_negatives = group_data[group_data[y_true_col] == 0]\n",
    "        \n",
    "        tpr = actual_positives['adjusted_pred'].mean() if len(actual_positives) > 0 else 0\n",
    "        fpr = actual_negatives['adjusted_pred'].mean() if len(actual_negatives) > 0 else 0\n",
    "        \n",
    "        tpr_rates.append(tpr)\n",
    "        fpr_rates.append(fpr)\n",
    "        \n",
    "        race_name = \"white\" if race == 1 else \"non-white\"\n",
    "        results[race_name] = {\n",
    "            'tpr': float(tpr),\n",
    "            'fpr': float(fpr),\n",
    "            'threshold': float(thresholds.get(race, 0.5)),\n",
    "            'count': int(len(group_data))\n",
    "        }\n",
    "    \n",
    "    max_tpr, min_tpr = max(tpr_rates), min(tpr_rates)\n",
    "    max_fpr, min_fpr = max(fpr_rates), min(fpr_rates)\n",
    "    \n",
    "    tpr_ratio = max_tpr / min_tpr if min_tpr > 0 else float('inf')\n",
    "    fpr_ratio = max_fpr / min_fpr if min_fpr > 0 else float('inf')\n",
    "    \n",
    "    results['tpr_ratio'] = float(tpr_ratio)\n",
    "    results['fpr_ratio'] = float(fpr_ratio)\n",
    "    results['equalized_odds_ratio'] = float(max(tpr_ratio, fpr_ratio))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Equalized Odds (with group-specific thresholds):\n",
      "\n",
      "Thresholds: non-white=0.31, white=0.5\n",
      "\n",
      "Equalized Odds Ratio: 1.1697\n",
      "TPR Ratio: 1.0474\n",
      "FPR Ratio: 1.1697\n",
      "\n",
      "By Group:\n",
      "  non-white: TPR=0.6919, FPR=0.3519, threshold=0.31, count=3756\n",
      "  white: TPR=0.7247, FPR=0.4116, threshold=0.5, count=6244\n"
     ]
    }
   ],
   "source": [
    "# Get predicted probabilities\n",
    "y_proba = best_pipe.predict_proba(X_val)[:, 1]\n",
    "# Define group-specific thresholds to try to equalize odds\n",
    "# Key: 0 = non-white, 1 = white\n",
    "# Adjust these thresholds to balance TPR/FPR across groups\n",
    "thresholds = {\n",
    "    0: 0.31,  # lower threshold for non-white (increases their TPR)\n",
    "    1: 0.5   # standard threshold for white\n",
    "}\n",
    "\n",
    "# Compute adjusted equalized odds\n",
    "results = adjusted_equalized_odds(val_df, target, y_proba, thresholds)\n",
    "\n",
    "print(\"Adjusted Equalized Odds (with group-specific thresholds):\")\n",
    "print(f\"\\nThresholds: non-white={thresholds[0]}, white={thresholds[1]}\")\n",
    "print(f\"\\nEqualized Odds Ratio: {results['equalized_odds_ratio']:.4f}\")\n",
    "print(f\"TPR Ratio: {results['tpr_ratio']:.4f}\")\n",
    "print(f\"FPR Ratio: {results['fpr_ratio']:.4f}\")\n",
    "print(\"\\nBy Group:\")\n",
    "for group in ['non-white', 'white']:\n",
    "    m = results[group]\n",
    "    print(f\"  {group}: TPR={m['tpr']:.4f}, FPR={m['fpr']:.4f}, threshold={m['threshold']}, count={m['count']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
