{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run **/other_files/task2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Baseline Model Comparison (Logistic Regression)\n",
    "\n",
    "In this notebook, you will train a Logistic Regression model to predict the `prior_hiring_decision` target variable. You will also begin your fairness analysis by establishing a baseline for accuracy and fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (30000, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Data\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "val_df = pd.read_csv('../data/val.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "\n",
    "columns_to_drop = [\"Age\", \"Sex\", \"Race\", \"Place_Of_Birth\", \"Hours_Per_Week\", \"Marital_Status\", \"Relationship\"]\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "print(\"Training Shape:\", train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and features\n",
    "target = 'prior_hiring_decision'\n",
    "features = [col for col in train_df.columns if col != target]\n",
    "\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target]\n",
    "X_val = val_df[features]\n",
    "y_val = val_df[target]\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target]\n",
    "\n",
    "# Define column types\n",
    "categorical_cols = ['Workclass', 'Sex', 'Race', 'Marital_Status', 'Education', 'Occupation', 'Relationship', 'Place_Of_Birth']\n",
    "numerical_cols = [col for col in features if col not in categorical_cols]\n",
    "\n",
    "# Preprocessing: scale numerical, one-hot encode categorical\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['saga']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intersectional_weights(df, protected_cols, label_col):\n",
    "    \"\"\"\n",
    "    Compute weights for intersectional groups (combinations of all protected characteristics).\n",
    "    W_{group, label} = (Count(Group) Ã— Count(Label)) / (Total Ã— Count(Group âˆ© Label))\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    labels = df[label_col].unique()\n",
    "    \n",
    "    # Create intersectional group column\n",
    "    df = df.copy()\n",
    "    df['_group'] = df[protected_cols].astype(str).agg('_'.join, axis=1)\n",
    "    \n",
    "    groups = df['_group'].unique()\n",
    "    \n",
    "    weights = {}\n",
    "    for group in groups:\n",
    "        group_count = len(df[df['_group'] == group])\n",
    "        for label in labels:\n",
    "            label_count = len(df[df[label_col] == label])\n",
    "            intersection_count = len(df[(df['_group'] == group) & (df[label_col] == label)])\n",
    "            \n",
    "            if intersection_count > 0:\n",
    "                weight = (group_count * label_count) / (total * intersection_count)\n",
    "            else:\n",
    "                weight = 1.0\n",
    "            \n",
    "            weights[(group, int(label))] = float(weight)\n",
    "    \n",
    "    return weights, df['_group']\n",
    "\n",
    "def compute_sample_weights(df, protected_cols, label_col):\n",
    "    \"\"\"\n",
    "    Compute per-sample weights based on intersectional group-label combinations.\n",
    "    \"\"\"\n",
    "    group_weights, group_col = compute_intersectional_weights(df, protected_cols, label_col)\n",
    "    \n",
    "    sample_weights = []\n",
    "    for idx, row in df.iterrows():\n",
    "        group_key = '_'.join([str(row[col]) for col in protected_cols])\n",
    "        label = int(row[label_col])\n",
    "        weight = group_weights.get((group_key, label), 1.0)\n",
    "        sample_weights.append(weight)\n",
    "    \n",
    "    return np.array(sample_weights), group_weights\n",
    "\n",
    "# Compute intersectional weights\n",
    "protected_cols = ['Sex', 'Race']\n",
    "target = 'prior_hiring_decision'\n",
    "\n",
    "sample_weights, group_weights = compute_sample_weights(train_df, protected_cols, target)\n",
    "\n",
    "print(\"Intersectional Group Weights:\")\n",
    "for (group, label), weight in sorted(group_weights.items()):\n",
    "    print(f\"  Group={group}, Label={label}: {weight:.4f}\")\n",
    "\n",
    "print(f\"\\nSample weights computed for {len(train_df)} samples\")\n",
    "print(f\"Weight range: [{sample_weights.min():.4f}, {sample_weights.max():.4f}]\")\n",
    "print(f\"Unique weights: {len(set(sample_weights))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for C in param_grid['C']:\n",
    "    for penalty in param_grid['penalty']:\n",
    "        clf = LogisticRegression(\n",
    "            C=C,\n",
    "            penalty=penalty,\n",
    "            solver='saga',\n",
    "            max_iter=1000,\n",
    "            random_state=42\n",
    "        )\n",
    "        clf.fit(X_train_processed, y_train)\n",
    "        score = clf.score(X_val_processed, y_val)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = {'C': C, 'penalty': penalty}\n",
    "\n",
    "print(f\"Best params: {best_params}\")\n",
    "print(f\"Best validation accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Train final model\n",
    "best_clf = LogisticRegression(**best_params, solver='saga', max_iter=1000, random_state=42)\n",
    "best_clf.fit(X_train_processed, y_train)\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', best_clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7316\n",
      "Confusion Matrix:\n",
      "[[3924 1275]\n",
      " [1409 3392]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewwicker/Desktop/Development/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/matthewwicker/Desktop/Development/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/matthewwicker/Desktop/Development/.venv/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fairness Metric Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_characteristics = ['Sex', 'Race', 'Age']\n",
    "\n",
    "# Add predictions to validation data for fairness evaluation\n",
    "val_with_preds = val_df.copy()\n",
    "val_with_preds['prediction'] = best_clf.predict(X_val_processed)\n",
    "\n",
    "def compute_fairness_metrics(data, protected_characteristics, target='prediction'):\n",
    "    results = {\n",
    "        'demographic_parity': {},\n",
    "        'equalized_opportunity': {},\n",
    "        'equalized_odds': {}\n",
    "    }\n",
    "    \n",
    "    for char in protected_characteristics:\n",
    "        groups = data[char].unique()\n",
    "        \n",
    "        # Demographic Parity\n",
    "        dp_info = {}\n",
    "        for group in groups:\n",
    "            group_data = data[data[char] == group]\n",
    "            dp_info[int(group)] = {\n",
    "                'positive_rate': float(group_data[target].mean()),\n",
    "                'count': int(len(group_data))\n",
    "            }\n",
    "        dp_info['ratio'] = float(demographic_parity(char, target, data))\n",
    "        results['demographic_parity'][char] = dp_info\n",
    "        \n",
    "        # Equalized Opportunity\n",
    "        eo_info = {}\n",
    "        for group in groups:\n",
    "            group_data = data[data[char] == group]\n",
    "            positive_cases = group_data[group_data[target] == 1]\n",
    "            eo_info[int(group)] = {\n",
    "                'tpr': float(positive_cases[target].mean()) if len(positive_cases) > 0 else None,\n",
    "                'positive_count': int(len(positive_cases))\n",
    "            }\n",
    "        eo_info['ratio'] = float(equalized_opportunity(char, target, data))\n",
    "        results['equalized_opportunity'][char] = eo_info\n",
    "        \n",
    "        # Equalized Odds\n",
    "        eod_info = {}\n",
    "        for group in groups:\n",
    "            group_data = data[data[char] == group]\n",
    "            positive_cases = group_data[group_data[target] == 1]\n",
    "            negative_cases = group_data[group_data[target] == 0]\n",
    "            eod_info[int(group)] = {\n",
    "                'tpr': float(positive_cases[target].mean()) if len(positive_cases) > 0 else None,\n",
    "                'fpr': float(negative_cases[target].mean()) if len(negative_cases) > 0 else None,\n",
    "            }\n",
    "        eod_info['ratio'] = float(equalized_odds(char, target, data))\n",
    "        results['equalized_odds'][char] = eod_info\n",
    "    \n",
    "    return results\n",
    "\n",
    "fairness_results = compute_fairness_metrics(val_with_preds, protected_characteristics, 'prediction')\n",
    "print(\"Fairness Metrics (Logistic Regression):\")\n",
    "print(\"\\nDemographic Parity:\", fairness_results['demographic_parity'])\n",
    "print(\"\\nEqualized Opportunity:\", fairness_results['equalized_opportunity'])\n",
    "print(\"\\nEqualized Odds:\", fairness_results['equalized_odds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ROC Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "y_proba = best_clf.predict_proba(X_val_processed)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_proba)\n",
    "auc_score = roc_auc_score(y_val, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {auc_score:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Logistic Regression')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adjusted Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_equalized_odds(data, protected_col, y_true_col, y_proba, thresholds):\n",
    "    \"\"\"\n",
    "    Calculate equalized odds using group-specific classification thresholds.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with protected characteristic and true labels\n",
    "        protected_col: Name of protected characteristic column\n",
    "        y_true_col: Name of true label column\n",
    "        y_proba: Array of predicted probabilities\n",
    "        thresholds: Dict mapping group values to classification thresholds\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    data['y_proba'] = y_proba\n",
    "    \n",
    "    data['adjusted_pred'] = data.apply(\n",
    "        lambda row: 1 if row['y_proba'] >= thresholds.get(row[protected_col], 0.5) else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    groups = data[protected_col].unique()\n",
    "    tpr_rates = []\n",
    "    fpr_rates = []\n",
    "    results = {}\n",
    "    \n",
    "    for group in groups:\n",
    "        group_data = data[data[protected_col] == group]\n",
    "        positive_cases = group_data[group_data[y_true_col] == 1]\n",
    "        negative_cases = group_data[group_data[y_true_col] == 0]\n",
    "        \n",
    "        tpr = positive_cases['adjusted_pred'].mean() if len(positive_cases) > 0 else 0\n",
    "        fpr = negative_cases['adjusted_pred'].mean() if len(negative_cases) > 0 else 0\n",
    "        \n",
    "        tpr_rates.append(tpr)\n",
    "        fpr_rates.append(fpr)\n",
    "        \n",
    "        results[int(group)] = {\n",
    "            'tpr': float(tpr),\n",
    "            'fpr': float(fpr),\n",
    "            'threshold': float(thresholds.get(group, 0.5)),\n",
    "            'count': int(len(group_data))\n",
    "        }\n",
    "    \n",
    "    max_tpr, min_tpr = max(tpr_rates), min(tpr_rates)\n",
    "    max_fpr, min_fpr = max(fpr_rates), min(fpr_rates)\n",
    "    \n",
    "    tpr_ratio = max_tpr / min_tpr if min_tpr > 0 else float('inf')\n",
    "    fpr_ratio = max_fpr / min_fpr if min_fpr > 0 else float('inf')\n",
    "    \n",
    "    results['tpr_ratio'] = float(tpr_ratio)\n",
    "    results['fpr_ratio'] = float(fpr_ratio)\n",
    "    results['equalized_odds_ratio'] = float(max(tpr_ratio, fpr_ratio))\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
