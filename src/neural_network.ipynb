{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Baseline Model Comparison (Neural Network)\n",
    "\n",
    "In this notebook, you will train a simple Neural Network (MLP) to predict the `prior_hiring_decision` target variable. Compare its performance to the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Data\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "val_df = pd.read_csv('../data/val.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "\n",
    "print(\"Training Shape:\", train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Preprocessing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define target and features\ntarget = 'prior_hiring_decision'\nfeatures = [col for col in train_df.columns if col != target]\n\nX_train = train_df[features]\ny_train = train_df[target]\nX_val = val_df[features]\ny_val = val_df[target]\nX_test = test_df[features]\ny_test = test_df[target]\n\n# Define column types\ncategorical_cols = ['Workclass', 'Sex', 'Race', 'Marital_Status', 'Education', 'Occupation', 'Relationship', 'Place_Of_Birth']\nnumerical_cols = [col for col in features if col not in categorical_cols]\n\n# Preprocessing: scale numerical, one-hot encode categorical\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n    ]\n)\n\n# Preprocess data\nX_train_processed = preprocessor.fit_transform(X_train)\nX_val_processed = preprocessor.transform(X_val)\n\n# Hyperparameter grid\nparam_grid = {\n    'hidden_layer_sizes': [(64,), (128,), (64, 32), (128, 64)],\n    'alpha': [0.0001, 0.001, 0.01],\n    'learning_rate_init': [0.001, 0.01]\n}\n\nbest_score = 0\nbest_params = {}\n\nfor hidden in param_grid['hidden_layer_sizes']:\n    for alpha in param_grid['alpha']:\n        for lr in param_grid['learning_rate_init']:\n            clf = MLPClassifier(\n                hidden_layer_sizes=hidden,\n                alpha=alpha,\n                learning_rate_init=lr,\n                max_iter=500,\n                random_state=42,\n                early_stopping=True\n            )\n            clf.fit(X_train_processed, y_train)\n            score = clf.score(X_val_processed, y_val)\n            if score > best_score:\n                best_score = score\n                best_params = {'hidden_layer_sizes': hidden, 'alpha': alpha, 'learning_rate_init': lr}\n\nprint(f\"Best params: {best_params}\")\nprint(f\"Best validation accuracy: {best_score:.4f}\")\n\n# Train final model\nbest_clf = MLPClassifier(**best_params, max_iter=500, random_state=42, early_stopping=True)\nbest_clf.fit(X_train_processed, y_train)\n\npipe = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', best_clf)\n])"
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Preprocessing Fairness",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_intersectional_weights(df, protected_cols, label_col):\n    total = len(df)\n    labels = df[label_col].unique()\n    \n    df = df.copy()\n    df['_group'] = df[protected_cols].astype(str).agg('_'.join, axis=1)\n    groups = df['_group'].unique()\n    \n    weights = {}\n    for group in groups:\n        group_count = len(df[df['_group'] == group])\n        for label in labels:\n            label_count = len(df[df[label_col] == label])\n            intersection_count = len(df[(df['_group'] == group) & (df[label_col] == label)])\n            \n            if intersection_count > 0:\n                weight = (group_count * label_count) / (total * intersection_count)\n            else:\n                weight = 1.0\n            \n            weights[(group, int(label))] = float(weight)\n    \n    return weights, df['_group']\n\ndef compute_sample_weights(df, protected_cols, label_col):\n    group_weights, group_col = compute_intersectional_weights(df, protected_cols, label_col)\n    \n    sample_weights = []\n    for idx, row in df.iterrows():\n        group_key = '_'.join([str(row[col]) for col in protected_cols])\n        label = int(row[label_col])\n        weight = group_weights.get((group_key, label), 1.0)\n        sample_weights.append(weight)\n    \n    return np.array(sample_weights), group_weights\n\nprotected_cols = ['Sex', 'Race']\nsample_weights, group_weights = compute_sample_weights(train_df, protected_cols, target)\n\nprint(\"Intersectional Group Weights:\")\nfor (group, label), weight in sorted(group_weights.items()):\n    print(f\"  Group={group}, Label={label}: {weight:.4f}\")\n\nprint(f\"\\nSample weights computed for {len(train_df)} samples\")\nprint(f\"Weight range: [{sample_weights.min():.4f}, {sample_weights.max():.4f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Fairness Metric Implementation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%run ../other_files/task2.ipynb\n\nprotected_characteristics = ['Sex', 'Race', 'Age']\n\nval_with_preds = val_df.copy()\nval_with_preds['prediction'] = best_clf.predict(X_val_processed)\n\ndef compute_fairness_metrics(data, protected_characteristics, target='prediction'):\n    results = {\n        'demographic_parity': {},\n        'equalized_opportunity': {},\n        'equalized_odds': {}\n    }\n    \n    for char in protected_characteristics:\n        groups = data[char].unique()\n        \n        dp_info = {}\n        for group in groups:\n            group_data = data[data[char] == group]\n            dp_info[int(group)] = {\n                'positive_rate': float(group_data[target].mean()),\n                'count': int(len(group_data))\n            }\n        dp_info['ratio'] = float(demographic_parity(char, target, data))\n        results['demographic_parity'][char] = dp_info\n        \n        eo_info = {}\n        for group in groups:\n            group_data = data[data[char] == group]\n            positive_cases = group_data[group_data[target] == 1]\n            eo_info[int(group)] = {\n                'tpr': float(positive_cases[target].mean()) if len(positive_cases) > 0 else None,\n                'positive_count': int(len(positive_cases))\n            }\n        eo_info['ratio'] = float(equalized_opportunity(char, target, data))\n        results['equalized_opportunity'][char] = eo_info\n        \n        eod_info = {}\n        for group in groups:\n            group_data = data[data[char] == group]\n            positive_cases = group_data[group_data[target] == 1]\n            negative_cases = group_data[group_data[target] == 0]\n            eod_info[int(group)] = {\n                'tpr': float(positive_cases[target].mean()) if len(positive_cases) > 0 else None,\n                'fpr': float(negative_cases[target].mean()) if len(negative_cases) > 0 else None,\n            }\n        eod_info['ratio'] = float(equalized_odds(char, target, data))\n        results['equalized_odds'][char] = eod_info\n    \n    return results\n\nfairness_results = compute_fairness_metrics(val_with_preds, protected_characteristics, 'prediction')\nprint(\"Fairness Metrics (Neural Network):\")\nprint(\"\\nDemographic Parity:\", fairness_results['demographic_parity'])\nprint(\"\\nEqualized Opportunity:\", fairness_results['equalized_opportunity'])\nprint(\"\\nEqualized Odds:\", fairness_results['equalized_odds'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. ROC Curve Analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\ny_proba = best_clf.predict_proba(X_val_processed)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_val, y_proba)\nauc_score = roc_auc_score(y_val, y_proba)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'Neural Network (AUC = {auc_score:.4f})')\nplt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve - Neural Network')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.show()\n\nprint(f\"AUC Score: {auc_score:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Adjusted Equalized Odds",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def adjusted_equalized_odds(data, protected_col, y_true_col, y_proba, thresholds):\n    \"\"\"\n    Calculate equalized odds using group-specific classification thresholds.\n    \n    Args:\n        data: DataFrame with protected characteristic and true labels\n        protected_col: Name of protected characteristic column\n        y_true_col: Name of true label column\n        y_proba: Array of predicted probabilities\n        thresholds: Dict mapping group values to classification thresholds\n    \"\"\"\n    data = data.copy()\n    data['y_proba'] = y_proba\n    \n    data['adjusted_pred'] = data.apply(\n        lambda row: 1 if row['y_proba'] >= thresholds.get(row[protected_col], 0.5) else 0,\n        axis=1\n    )\n    \n    groups = data[protected_col].unique()\n    tpr_rates = []\n    fpr_rates = []\n    results = {}\n    \n    for group in groups:\n        group_data = data[data[protected_col] == group]\n        positive_cases = group_data[group_data[y_true_col] == 1]\n        negative_cases = group_data[group_data[y_true_col] == 0]\n        \n        tpr = positive_cases['adjusted_pred'].mean() if len(positive_cases) > 0 else 0\n        fpr = negative_cases['adjusted_pred'].mean() if len(negative_cases) > 0 else 0\n        \n        tpr_rates.append(tpr)\n        fpr_rates.append(fpr)\n        \n        results[int(group)] = {\n            'tpr': float(tpr),\n            'fpr': float(fpr),\n            'threshold': float(thresholds.get(group, 0.5)),\n            'count': int(len(group_data))\n        }\n    \n    max_tpr, min_tpr = max(tpr_rates), min(tpr_rates)\n    max_fpr, min_fpr = max(fpr_rates), min(fpr_rates)\n    \n    tpr_ratio = max_tpr / min_tpr if min_tpr > 0 else float('inf')\n    fpr_ratio = max_fpr / min_fpr if min_fpr > 0 else float('inf')\n    \n    results['tpr_ratio'] = float(tpr_ratio)\n    results['fpr_ratio'] = float(fpr_ratio)\n    results['equalized_odds_ratio'] = float(max(tpr_ratio, fpr_ratio))\n    \n    return results",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}